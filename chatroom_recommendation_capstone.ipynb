{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Learning Nanodegree Capstone\n",
    "# Chat Room Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path will need to be changed pending on where the repo is cloned to\n",
    "os.chdir(os.path.expanduser('~/PycharmProjects/chat-room-recommendation/'))\n",
    "lines = open('cornell-movie-dialogs-corpus/movie_lines.txt','r').read().split('\\n')\n",
    "conv_lines = open('cornell-movie-dialogs-corpus/movie_conversations.txt','r').read().split('\\n')\n",
    "character_metadata = open('cornell-movie-dialogs-corpus/movie_characters_metadata.txt','r').read().split('\\n')\n",
    "movie_metadata = open('cornell-movie-dialogs-corpus/movie_titles_metadata.txt','r').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\",\n 'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow',\n \"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\",\n 'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No',\n 'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?',\n 'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\",\n \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\",\n \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\",\n \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\",\n \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\",\n \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_lines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gensim simple pre-processing tool to create a dictionary with keys = movie_id and value = tokenized text of all the lines in the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "movieLines = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        if _line[2] in movieLines: \n",
    "            movieLines[_line[2]] = movieLines.get(_line[2]) + utils.simple_preprocess(_line[4])\n",
    "        else:\n",
    "            movieLines[_line[2]] = utils.simple_preprocess(_line[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct # of movies\n"
     ]
    }
   ],
   "source": [
    "# sanity check that there are the appropriate number of movies in the movieLines dict\n",
    "print \"Correct # of movies\" if len(movieLines) == 617 else \"something went wrong with movieLines dict\"\n",
    "# print movieLines.get(\"m616\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4902"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find average length of tokenized text for the movies.\n",
    "totalLines = 0\n",
    "for key, value in movieLines.iteritems():\n",
    "    totalLines = totalLines + len(value)\n",
    "    \n",
    "totalLines/len(movieLines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary to map each line's id with it's text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct # of lines\n"
     ]
    }
   ],
   "source": [
    "# sanity check for id2line dict\n",
    "print \"Correct # of lines\" if len(id2line) == 304713 else \"something went wrong with id2line dict\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gensim simple pre-processing tool to create a dictionary with keys = character_id and value = tokenized text of all conversations for that character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "characterConversations = {}\n",
    "for line in conv_lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 4:\n",
    "        _line[3] = _line[3].strip(\"[]\")\n",
    "        for conv in _line[3].split(\",\"):\n",
    "            conv = conv.replace(\"'\",\"\").replace(\" \", \"\")\n",
    "            if _line[0] in characterConversations:\n",
    "                characterConversations[_line[0]] = characterConversations.get(_line[0]) + \\\n",
    "                                                   utils.simple_preprocess(id2line.get(str(conv)))\n",
    "            else:\n",
    "                characterConversations[_line[0]] = utils.simple_preprocess(id2line.get(str(conv)))\n",
    "\n",
    "            if _line[1] in characterConversations:\n",
    "                characterConversations[_line[1]] = characterConversations.get(_line[1]) + \\\n",
    "                                                   utils.simple_preprocess(id2line.get(str(conv)))\n",
    "            else:\n",
    "                characterConversations[_line[1]] = utils.simple_preprocess(id2line.get(str(conv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct # of characters\n"
     ]
    }
   ],
   "source": [
    "# sanity check that there are the appropriate # of characters in the characterConversation dict\n",
    "print \"Correct # of characters\" if len(characterConversations) == 9035 else \"something went wrong with character dict\"\n",
    "#print characterConversations.get(\"u0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "669"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find average length of tokenized text for each character.\n",
    "totalLines = 0\n",
    "for key, value in characterConversations.iteritems():\n",
    "    totalLines = totalLines + len(value)\n",
    "    \n",
    "totalLines/len(characterConversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that creates corpus (list of TaggedDocmuments) from dictionaries\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "\n",
    "def create_corpus(dictname):\n",
    "    corpus_list =[]\n",
    "    for key, value in dictname.iteritems():\n",
    "        corpus_list.append(TaggedDocument(value, [int(key[1:])]))\n",
    "    return corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = create_corpus(movieLines)\n",
    "test_corpus = create_corpus(characterConversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct # of movies in train_corpus\nCorrect # of characters in test_corpus\n"
     ]
    }
   ],
   "source": [
    "# sanity check of length of corpus\n",
    "print \"Correct # of movies in train_corpus\" if len(train_corpus) == 617 else \"something went wrong with train_corpus\"\n",
    "print \"Correct # of characters in test_corpus\" if len(test_corpus) == 9035 else \"something went wrong with test_corpus\"\n",
    "# print train_corpus[0].tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a Doc2Vec Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 24s, sys: 1.45 s, total: 2min 25s\nWall time: 55.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44390174"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "\n",
    "model = Doc2Vec(size=50, iter=20, min_count=2)\n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model if saved during a previous session\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "model = Doc2Vec.load('/tmp/movie_model.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Model\n",
    "To assess the doc2vec model, inferring new vectors for each document of the training corpus, compare the inferred vectors with the training corpus, and then returning the rank of the document based on self-similarity. This approach is pretending as if the training corpus is some new unseen data and then seeing how they compare with the trained model. The expectation is that model will be overfit and so finding similar documents should be very easily. The second ranks will also be tracked for comparison of less similar docutents. (https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "first_ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    \n",
    "    rank = [docid for docid, sim in sims].index(train_corpus[doc_id].tags[0])\n",
    "        \n",
    "    ranks.append(rank) \n",
    "    \n",
    "    first_ranks.append(sims[0])\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 613, 1: 4})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get character metadata by id\n",
    "def get_character_metadata(id):\n",
    "    for character in character_metadata:\n",
    "        _character = character.split(' +++$+++ ')\n",
    "        if len(_character) == 6:\n",
    "            if _character[0] == 'u'+str(id):\n",
    "                return _character\n",
    "        \n",
    "           \n",
    "def get_movie_title(id):\n",
    "    for movie in movie_metadata:\n",
    "        _movie = movie.split(' +++$+++ ')\n",
    "        if len(_movie) == 6:\n",
    "            if _movie[0] == 'm'+str(id):\n",
    "                return _movie[1]\n",
    "            \n",
    "            \n",
    "# create def to look up tag doc in corpus by tag id\n",
    "def get_corpus_index(corpus, tag):\n",
    "    for tag_index in range(len(corpus)):\n",
    "        if corpus[tag_index].tags[0] == tag:\n",
    "            return tag_index\n",
    "     \n",
    "     \n",
    "def get_movie_genres(id):\n",
    "    for movie in movie_metadata:\n",
    "        _movie = movie.split(' +++$+++ ')\n",
    "        if len(_movie) == 6:\n",
    "            if _movie[0] == 'm'+str(id):\n",
    "                return _movie[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "Mean Average Precision (MAP) will be used to test the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find similarity of movie character is from and index \n",
    "def get_source_similarity(sim_list, movie_id):\n",
    "    for sim_index in range(len(sim_list)):\n",
    "        if int(movie_id[1:]) == sim_list[sim_index][0]:\n",
    "            return get_movie_title(sim_list[sim_index][0]), sim_index, sim_list[sim_index][1]\n",
    "        \n",
    "        \n",
    "# print top 5 recommended movies\n",
    "def get_recommended_movies(sim_list):\n",
    "    print('\\nTop 5\\n {}\\n {}\\n {}\\n {}\\n {}\\n'.format(get_movie_title(sim_list[0][0]), get_movie_title(sim_list[1][0]), \n",
    "                                                get_movie_title(sim_list[2][0]), get_movie_title(sim_list[3][0]),\n",
    "                                                get_movie_title(sim_list[4][0])))\n",
    "    \n",
    "    \n",
    "def display_character_similarity(char_id, show_words):\n",
    "    inferred_vector = model.infer_vector(test_corpus[char_id].words)\n",
    "    test_sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    character_metadata = get_character_metadata(char_id)\n",
    "    print('Character : {}\\n ID : {}\\n Movie : {}'.format(character_metadata[1], character_metadata[2], \n",
    "                                                         character_metadata[3]))\n",
    "    if show_words :\n",
    "        print('Test Document ({}): «{}»\\n'.format(char_id, ' '.join(test_corpus[get_corpus_index(test_corpus, char_id)].words)))\n",
    "    print test_sims[:10]\n",
    "    get_recommended_movies(test_sims)\n",
    "    print get_source_similarity(test_sims, character_metadata[2]), '\\n'\n",
    "    \n",
    "    \n",
    "def get_doc2vec_similarity(char_id, test_model):\n",
    "    inferred_vector = test_model.infer_vector(test_corpus[char_id].words)\n",
    "    test_sims = test_model.docvecs.most_similar([inferred_vector], topn=len(test_model.docvecs))\n",
    "\n",
    "    return test_sims[:20]\n",
    "\n",
    "\n",
    "def precision_at_k(r, k):    \n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        raise ValueError('Relevance score length < k')\n",
    "    return np.mean(r)\n",
    "\n",
    "\n",
    "def calc_average_precision(recommendations, movie_id):\n",
    "    metadata = get_character_metadata(movie_id)\n",
    "    genres_str = get_movie_genres(metadata[2][1:])\n",
    "    # print genres_str\n",
    "    \n",
    "    genres_str = genres_str.strip(\"[]\").replace(\"'\",\"\").replace(\" \", \"\")\n",
    "    genre_list = genres_str.split(\",\")  \n",
    "    predicted_list = []\n",
    "    for item in recommendations:\n",
    "        # print get_movie_genres(item[0])\n",
    "        is_relevant = False\n",
    "        for gen in genre_list:\n",
    "            if gen in get_movie_genres(item[0]):\n",
    "                if is_relevant is False:\n",
    "                    # print \"+\"\n",
    "                    is_relevant = True\n",
    "        if is_relevant:\n",
    "            predicted_list.append(1)\n",
    "        else:\n",
    "            predicted_list.append(0)\n",
    "            \n",
    "    # print predicted_list\n",
    "    \n",
    "    predicted_list = np.asarray(predicted_list) != 0\n",
    "    out = [precision_at_k(predicted_list, k + 1) for k in range(predicted_list.size) if predicted_list[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "\n",
    "def calc_mean_average_precision_k(test_list, test_model):\n",
    "    av_list = []\n",
    "    for char in test_list:\n",
    "        d2v_result = get_doc2vec_similarity(char, test_model)\n",
    "        av_list.append(calc_average_precision(d2v_result, char))\n",
    "    \n",
    "    return np.mean(av_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random test data to be used - Of the 9035 character 500 will be randomly added to a list for testing\n",
    "test_character_list = []\n",
    "for _ in range(500):\n",
    "    rand_id = random.randint(0, len(test_corpus))\n",
    "    test_character_list.append(rand_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64590552578393612"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_mean_average_precision_k(test_character_list[:200], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA0aomsVeJbn9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09601921d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_sizes_list = [10, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "mapk_results = []\n",
    "for test_size in test_sizes_list:\n",
    "    mapk_results.append(calc_mean_average_precision_k(test_character_list[:test_size], model))\n",
    "    \n",
    "\n",
    "plt.plot(test_sizes_list, mapk_results)\n",
    "plt.axis([0, 500, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.607888950913\n"
     ]
    }
   ],
   "source": [
    "# final 4\n",
    "model = Doc2Vec(alpha=0.25, min_alpha=0.025) \n",
    "model.build_vocab(train_corpus)\n",
    "# %time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "for epoch in range(10):\n",
    "    model.train(train_corpus, total_examples=model.corpus_count, epochs=epoch)\n",
    "    model.alpha -= 0.002  # decrease the learning rate\n",
    "    model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "    \n",
    "print calc_mean_average_precision_k(test_character_list[:200], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 33s, sys: 632 ms, total: 1min 34s\nWall time: 34.1 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.643467036911\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(dm=1, dm_mean=1, size=200, window=8, min_count=19, iter=10) \n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print calc_mean_average_precision_k(test_character_list[:200], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 10s, sys: 2.15 s, total: 3min 12s\nWall time: 1min 13s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.641963598382\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(dm=1, dm_mean=1, size=200, window=8, min_count=19, iter=20) \n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print calc_mean_average_precision_k(test_character_list[:200], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 54s, sys: 1.18 s, total: 3min 55s\nWall time: 1min 22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.649278035546\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(dm=1, dm_mean=1, size=300, window=8, min_count=19, iter=20) \n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print calc_mean_average_precision_k(test_character_list[:200], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 22s, sys: 6.62 s, total: 15min 28s\nWall time: 5min 35s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.635627557614\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(dm=1, dm_mean=1, size=200, window=8, min_count=19, iter=100) \n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print calc_mean_average_precision_k(test_character_list[:200], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 46s, sys: 4.78 s, total: 21min 51s\nWall time: 7min 32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6184851441\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(dm=1, dm_mean=1, size=200, window=20, min_count=19, iter=100) \n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print calc_mean_average_precision_k(test_character_list[:200], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 10s, sys: 5.4 s, total: 13min 15s\nWall time: 4min 51s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.647298604327\n"
     ]
    }
   ],
   "source": [
    "# final 4\n",
    "model = Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=19, iter=100) \n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print calc_mean_average_precision_k(test_character_list[:200], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 5s, sys: 5.07 s, total: 13min 10s\nWall time: 4min 49s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.647055708097\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=20, iter=100) \n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print calc_mean_average_precision_k(test_character_list[:200], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 43s, sys: 6.3 s, total: 13min 50s\nWall time: 5min 4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.641714744505\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=10, iter=100) \n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print calc_mean_average_precision_k(test_character_list[:200], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 22s, sys: 8.45 s, total: 13min 31s\nWall time: 5min 4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.645888061985\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=15, iter=100) \n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print calc_mean_average_precision_k(test_character_list[:200], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 25s, sys: 6.76 s, total: 10min 32s\nWall time: 4min 10s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64739936922\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=15, iter=100, alpha=0.25, min_alpha=0.01) \n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print calc_mean_average_precision_k(test_character_list[:200], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 2s, sys: 15 s, total: 20min 17s\nWall time: 7min 44s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.619167139798\n"
     ]
    }
   ],
   "source": [
    "# final 4\n",
    "model = Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=15, iter=150) \n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print calc_mean_average_precision_k(test_character_list[:200], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/m,d200,n5,w4,mc15,s0.001,t3)\n"
     ]
    }
   ],
   "source": [
    "print model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26min 26s, sys: 15.7 s, total: 26min 41s\nWall time: 9min 59s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.619998446705\n"
     ]
    }
   ],
   "source": [
    "# final 4\n",
    "model = Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=15, iter=200) \n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print calc_mean_average_precision_k(test_character_list[:200], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39min 9s, sys: 22.4 s, total: 39min 31s\nWall time: 14min 46s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.644189477281\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=15, iter=300) \n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print calc_mean_average_precision_k(test_character_list[:200], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 8s, sys: 5.06 s, total: 13min 13s\nWall time: 4min 49s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64917898177\n"
     ]
    }
   ],
   "source": [
    "# try to run against entire character test\n",
    "character_list = []\n",
    "for id in range(len(test_corpus)):\n",
    "    character_list.append(id)\n",
    "\n",
    "model = Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=19, iter=100) \n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print calc_mean_average_precision_k(character_list, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to be loaded later if needed\n",
    "model.save('/tmp/movie_model.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results\n",
    "Round 1\n",
    "Doc2Vec(size=50, iter=20, min_count=2) : 0.64590552578393612<br/>\n",
    "Doc2Vec(alpha=0.025, min_alpha=0.025)/decrease learning rate : 0.652129159832<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=8, min_count=19, iter=10) : 0.643467036911<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=8, min_count=19, iter=20) : 0.641963598382<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=300, window=8, min_count=19, iter=20) : 0.649278035546<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=8, min_count=19, iter=100) : 0.635627557614<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=19, iter=100) : 0.664204807617<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=20, iter=100) : 0.639810561736<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=10, iter=100) : 0.641714744505<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=15, iter=100) : 0.645888DD061985<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=15, iter=100, alpha=0.25, min_alpha=0.01) : 0.64739936922<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=15, iter=150) : 0.6501950632<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=15, iter=200) : 0.653454281673<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=15, iter=300) : 0.644189477281<br/>\n",
    "<br/>\n",
    "Round 2 - new random test data<br/>\n",
    "Doc2Vec(alpha=0.025, min_alpha=0.025)/decrease learning rate : 0.616014300118<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=19, iter=100) : 0.644970657008<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=15, iter=150) : 0.634117843088<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=15, iter=200) : 0.638209043472<br/>\n",
    "<br/>\n",
    "Round 3 - new random test data<br/>\n",
    "Doc2Vec(alpha=0.025, min_alpha=0.025)/decrease learning rate : 0.607888950913<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=19, iter=100) : 0.614642972259<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=15, iter=150) : 0.619167139798<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=15, iter=200) : 0.619998446705<br/>\n",
    "<br/>\n",
    "Average<br/>\n",
    "Doc2Vec(alpha=0.025, min_alpha=0.025)/decrease learning rate : (0.652129159832 + 0.616014300118 + 0.607888950913)/3 = 0.625344137<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=19, iter=100) : (0.664204807617 + 0.644970657008 + 0.614642972259)/3 = 0.641272812<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=15, iter=150) : (0.6501950632 + 0.634117843088 + 0.619167139798)/3 = 0.634493349<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=15, iter=200) : (0.653454281673+ 0.638209043472 + 0.619998446705)/3 = 0.637220591<br/>\n",
    "<br/>\n",
    "Round 4 - new random test data min_count 19 vs min_count 20<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=19, iter=100) : 0.654732378791<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=20, iter=100) : 0.649219429736<br/>\n",
    "<br/>\n",
    "Round 5 - same random test data min_count 19 vs min_count 20<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=19, iter=100) : 0.647298604327 <br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=20, iter=100) : 0.647055708097 <br/>\n",
    "<br/>\n",
    "MAP run with full test corpus<br/>\n",
    "Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=19, iter=100) : 0.64917898177"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity analysis\n",
    "Test Doc2Vec model with random lines removed from the training_corpus. The model will be assessed the same way the original training_corpus was assessed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# altlines = open('cornell-movie-dialogs-corpus/movie_lines_mod.txt','r').read().split('\\n')\n",
    "from random import randint\n",
    "from gensim import utils\n",
    "\n",
    "altMovieLines = {}\n",
    "altTotalLines = 0\n",
    "for line in lines:\n",
    "    if randint(0, 9) != 7:\n",
    "        altTotalLines = altTotalLines+1\n",
    "        _line = line.split(' +++$+++ ')\n",
    "        if len(_line) == 5:\n",
    "            if _line[2] in altMovieLines: \n",
    "                altMovieLines[_line[2]] = altMovieLines.get(_line[2]) + utils.simple_preprocess(_line[4])\n",
    "            else:\n",
    "                altMovieLines[_line[2]] = utils.simple_preprocess(_line[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct # of movies\n# of lines were decreased\n274412\n"
     ]
    }
   ],
   "source": [
    "print \"Correct # of movies\" if len(altMovieLines) == 617 else \"something went wrong with movieLines dict\"\n",
    "print \"# of lines were decreased\" if altTotalLines < 304713 else \"something went wrong with id2line dict\"\n",
    "print altTotalLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 26s, sys: 7.13 s, total: 20min 33s\nWall time: 8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.663162180496\n"
     ]
    }
   ],
   "source": [
    "alt_train_corpus = create_corpus(altMovieLines)\n",
    "alt_model = Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=19, iter=100)\n",
    "alt_model.build_vocab(alt_train_corpus)\n",
    "%time alt_model.train(alt_train_corpus, total_examples=alt_model.corpus_count, epochs=alt_model.iter)\n",
    "print calc_mean_average_precision_k(test_character_list[:200], alt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_model.save('/tmp/alt_model.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "\n",
    "alt_model = Doc2Vec.load('/tmp/alt_model.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Doc2Vec model with test_corpus (individual character converations) which consist of 9035 different documents.  The model will be assessed the same way the training_corpus was assessed.This is essentually recommending users to other users.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt MAP functions to use test_corpus to train.  \n",
    "def calc_average_precision2(recommendations, movie_id):\n",
    "    metadata = get_character_metadata(movie_id)\n",
    "    genres_str = get_movie_genres(metadata[2][1:])\n",
    "    # print genres_str\n",
    "    \n",
    "    genres_str = genres_str.strip(\"[]\").replace(\"'\",\"\").replace(\" \", \"\")\n",
    "    genre_list = genres_str.split(\",\")  \n",
    "    predicted_list = []\n",
    "    for item in recommendations:\n",
    "        # print get_movie_genres(item[0])\n",
    "        is_relevant = False\n",
    "        for gen in genre_list:\n",
    "            if gen in get_movie_genres(get_character_metadata(item[0])[2][1:]):\n",
    "                if is_relevant is False:\n",
    "                    # print \"+\"\n",
    "                    is_relevant = True\n",
    "        if is_relevant:\n",
    "            predicted_list.append(1)\n",
    "        else:\n",
    "            predicted_list.append(0)\n",
    "            \n",
    "    # print predicted_list\n",
    "    \n",
    "    predicted_list = np.asarray(predicted_list) != 0\n",
    "    out = [precision_at_k(predicted_list, k + 1) for k in range(predicted_list.size) if predicted_list[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "\n",
    "def calc_mean_average_precision_k2(test_list, test_model):\n",
    "    av_list = []\n",
    "    for char in test_list:\n",
    "        d2v_result = get_doc2vec_similarity(char, test_model)\n",
    "        av_list.append(calc_average_precision2(d2v_result, char))\n",
    "    \n",
    "    return np.mean(av_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43min 9s, sys: 28.6 s, total: 43min 37s\nWall time: 17min 36s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.608206265221\n"
     ]
    }
   ],
   "source": [
    "model2 = Doc2Vec(dm=1, dm_mean=1, size=200, window=4, min_count=19, iter=100)\n",
    "model2.build_vocab(test_corpus)\n",
    "%time model2.train(test_corpus, total_examples=model2.corpus_count, epochs=model2.iter)\n",
    "print calc_mean_average_precision_k2(test_character_list[:200], model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('/tmp/character_model.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "\n",
    "model2 = Doc2Vec.load('/tmp/character_model.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple test of the first movie in the training corpus to ensure that the movie was returned as the most similar document by the model.  The similarity score is also displayed. A random document from the training coupus is aslo selected along and compared with the similarity score of the second most similar document.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie title : star wars\nMovie id : 529\n\nMODEL Doc2Vec(dm/m,d200,n5,w4,mc19,s0.001,t3):\n\nstar wars\n(529, 0.9724684953689575) \n\n----------------------\n\nTrain Document (462): «notting hill»\n\nTop Document (462, 0.9851363897323608): «notting hill»\n\nSimilar Document (263, 0.3918589651584625): «bean»\n\n"
     ]
    }
   ],
   "source": [
    "print('Movie title : {}\\nMovie id : {}\\n'.format(get_movie_title(train_corpus[doc_id].tags[0]), train_corpus[doc_id].tags[0]))\n",
    "print(u'MODEL %s:\\n' % model)\n",
    "\n",
    "print get_movie_title(train_corpus[get_corpus_index(train_corpus, sims[0][0])].tags[0])\n",
    "print sims[0], '\\n\\n----------------------\\n'\n",
    "\n",
    "# Pick a random document from the train corpus and infer a vector from the model\n",
    "rand_train_id = random.randint(0, len(train_corpus))\n",
    "\n",
    "# Inspect the score of the second ranked movie. \n",
    "# The score for the second ranked movie should be much lower then for the top document.\n",
    "print('Train Document ({}): «{}»\\n'.format(rand_train_id, get_movie_title(rand_train_id)))\n",
    "top_id = first_ranks[get_corpus_index(train_corpus, rand_train_id)]\n",
    "print('Top Document {}: «{}»\\n'.format(top_id, get_movie_title(top_id[0])))\n",
    "sim_id = second_ranks[get_corpus_index(train_corpus, rand_train_id)]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, get_movie_title(sim_id[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character : EDDIE\n ID : m80\n Movie : halloween h20: 20 years later\n[(379, 0.6426482200622559), (499, 0.34675082564353943), (342, 0.34512200951576233), (188, 0.3361318111419678), (447, 0.332575261592865), (376, 0.33209237456321716), (212, 0.32587167620658875), (20, 0.3134957551956177), (359, 0.31340932846069336), (59, 0.31223708391189575)]\n\nTop 5\n halloween\n sleepless in seattle\n the fabulous baker boys\n someone to watch over me\n arcade\n\n('halloween h20: 20 years later', 66, 0.23796817660331726) \n\nAP@20 for character 1220 : 0.843616907688\nCharacter : KERI\n ID : m80\n Movie : halloween h20: 20 years later\n[(439, 0.5598339438438416), (276, 0.37651336193084717), (372, 0.3285962641239166), (425, 0.3180466890335083), (467, 0.31794071197509766), (281, 0.31404656171798706), (38, 0.30948635935783386), (154, 0.3086808919906616), (324, 0.3023444712162018), (231, 0.30112865567207336)]\n\nTop 5\n midnight express\n blow\n goodfellas\n lost in translation\n out of sight\n\n('halloween h20: 20 years later', 315, 0.1577019989490509) \n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP@20 for character 1223 : 0.866611021569\n"
     ]
    }
   ],
   "source": [
    "# Two characters from the same movie \n",
    "display_character_similarity(1220, False)\n",
    "print(\"AP@20 for character 1220 : {}\".format(calc_average_precision(get_doc2vec_similarity(1220, model), 1220)))\n",
    "display_character_similarity(1223, False)\n",
    "print(\"AP@20 for character 1223 : {}\".format(calc_average_precision(get_doc2vec_similarity(1223, model), 1223)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character : HAN\n ID : m529\n Movie : star wars\n[(92, 0.6449993848800659), (430, 0.3893139362335205), (65, 0.37343278527259827), (216, 0.3703673481941223), (158, 0.35381242632865906), (349, 0.3520643711090088), (222, 0.35035082697868347), (359, 0.3476943075656891), (488, 0.32488399744033813), (170, 0.3181003928184509)]\n\nTop 5\n house of 1000 corpses\n manhunt\n from dusk till dawn\n thelma & louise\n pet sematary\n\n('star wars', 487, 0.07965101301670074) \n\nCharacter : LUKE\n ID : m529\n Movie : star wars\n[(92, 0.8937079310417175), (217, 0.3398858308792114), (314, 0.33799755573272705), (237, 0.3355068862438202), (537, 0.33118876814842224), (61, 0.3258844316005707), (349, 0.323481947183609), (72, 0.32014745473861694), (41, 0.31066903471946716), (430, 0.3015619218349457)]\n\nTop 5\n house of 1000 corpses\n there's something about mary\n the curse\n alien vs. predator\n suburbia\n\n('star wars', 507, 0.0231881532818079) \n\nCharacter : VADER\n ID : m529\n Movie : star wars\nTest Document (7827): «enough already know about the data you ve intercepted but its too late whatever information you ve gathered will be destroyed you will come to know such suffering as only the master of the bogan force can provide you ll get no information from me you have no authority the council can hold me it appears your ship had an accident will see to it that your death is duely reported there will be no one to save you this time the death star has become operational there is no force in the universe that can stop us now they ll find its weakness it too late we already tested it on organa major it appears your data never got through no it would be much easier if you were to tell us where the outposts are otherwise we ll be forced to destroy every suspicious system what waste»\n\n[(356, 0.6148561835289001), (221, 0.3421282172203064), (200, 0.3161830008029938), (67, 0.2985665798187256), (496, 0.29640382528305054), (383, 0.2939368784427643), (148, 0.29295116662979126), (110, 0.2871250808238983), (25, 0.2785463035106659), (88, 0.27507349848747253)]\n\nTop 5\n the adventures of ford fairlane\n total recall\n strange days\n godzilla\n rush hour 2\n\n('star wars', 608, 0.012682280503213406) \n\nCharacter : BRIDGET\n ID : m479\n Movie : pretty woman\n[(388, 0.37903714179992676), (404, 0.30434757471084595), (118, 0.3015550374984741), (446, 0.2989054024219513), (1, 0.2894974648952484), (192, 0.2892962098121643), (235, 0.288332462310791), (615, 0.27254506945610046), (209, 0.2711416780948639), (191, 0.26842525601387024)]\n\nTop 5\n highlander iii: the sorcerer\n jaws 3-d\n legend\n mrs brown\n 1492: conquest of paradise\n\n('pretty woman', 335, 0.15378254652023315) \n\n"
     ]
    }
   ],
   "source": [
    "# compare results from three characters that have conversations in a movie and a random character\n",
    "# Star wars : Hans, Luke and Vader\n",
    "# u7821 +++$+++ HAN +++$+++ m529\n",
    "display_character_similarity(7821, False)\n",
    "\n",
    "# u7824 +++$+++ LUKE +++$+++ m529\n",
    "display_character_similarity(7824, False)\n",
    "\n",
    "# u7827 +++$+++ VADER\n",
    "display_character_similarity(7827, True)\n",
    "\n",
    "# random character from test corpus\n",
    "display_character_similarity(random.randint(0, len(test_corpus)), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character : PETER\n ID : m367\n Movie : get carter\nTest Document (5519): «gerald phoned us in the middle of the night said he heard you ve been making nuisance of yourself we ve got to take you back to london he said it be doing him big favour we know why you re all steamed up and so do gerald and sid but they have to be diplomatic put it away jack you know you won use it the gun he means gerald wants to see him first shut up»\n\n[(465, 0.50835782289505), (345, 0.3186812698841095), (58, 0.31676313281059265), (203, 0.3082112967967987), (270, 0.30003443360328674), (444, 0.2980826199054718), (388, 0.2907238006591797), (228, 0.2814142107963562), (24, 0.2809237539768219), (530, 0.2664611041545868)]\n\nTop 5\n on the waterfront\n the fantastic four\n fantastic four\n the godfather\n the black dahlia\n\n('get carter', 239, 0.14768920838832855) \n\nCharacter : CON\n ID : m367\n Movie : get carter\nTest Document (5508): «gerald phoned us in the middle of the night said he heard you ve been making nuisance of yourself we ve got to take you back to london he said it be doing him big favour we know why you re all steamed up and so do gerald and sid but they have to be diplomatic put it away jack you know you won use it the gun he means gerald wants to see him first shut up»\n\n[(602, 0.36127790808677673), (469, 0.346068799495697), (17, 0.33909812569618225), (349, 0.3369516134262085), (12, 0.334179550409317), (594, 0.3201464116573334), (318, 0.3099710941314697), (383, 0.30605003237724304), (362, 0.3050677478313446), (334, 0.30442503094673157)]\n\nTop 5\n what women want\n peggy sue got married\n an american werewolf in london\n final destination 2\n airplane ii: the sequel\n\n('get carter', 189, 0.22785599529743195) \n\n"
     ]
    }
   ],
   "source": [
    "# two characters that only have conversations with each other so they have identical words in their document\n",
    "display_character_similarity(5519, True)\n",
    "display_character_similarity(5508, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character : PAPAGENO\n ID : m16\n Movie : amadeus\nTest Document (259): «here am my angel what who the devil are you ve taken pity on you my angel heard your wish oh well thank you how wonderful some people get all the luck now you ve got to promise me faithfully you ll remain true to me forever then you ll see how tenderly your little birdie will love you can wait well promise then what do you mean now of course now right away before get any older well don know mean you re delicious delightful delectable little bird but don you think you might be just little tough oh tender enough for you my boy tender enough for you»\n\n[(369, 0.48146891593933105), (203, 0.3504234254360199), (494, 0.3287377655506134), (424, 0.32663020491600037), (272, 0.3177914023399353), (387, 0.31425905227661133), (613, 0.3100992441177368), (436, 0.3068705201148987), (309, 0.2976357638835907), (581, 0.2970328629016876)]\n\nTop 5\n the godfather: part ii\n the godfather\n ronin\n lord of illusions\n blade\n\n('amadeus', 83, 0.2444731742143631) \n\nCharacter : UGLY OLD WOMAN\n ID : m16\n Movie : amadeus\nTest Document (265): «here am my angel what who the devil are you ve taken pity on you my angel heard your wish oh well thank you how wonderful some people get all the luck now you ve got to promise me faithfully you ll remain true to me forever then you ll see how tenderly your little birdie will love you can wait well promise then what do you mean now of course now right away before get any older well don know mean you re delicious delightful delectable little bird but don you think you might be just little tough oh tender enough for you my boy tender enough for you»\n\n[(369, 0.3845692574977875), (203, 0.3796014189720154), (18, 0.37486979365348816), (166, 0.3734526038169861), (402, 0.347619891166687), (16, 0.3463020622730255), (568, 0.34541651606559753), (142, 0.33980417251586914), (563, 0.3323899209499359), (494, 0.3311367630958557)]\n\nTop 5\n the godfather: part ii\n the godfather\n american madness\n raging bull\n it's a wonderful life\n\n('amadeus', 5, 0.3463020622730255) \n\n"
     ]
    }
   ],
   "source": [
    "# two characters that only have conversations with each other so they have identical words in their document\n",
    "display_character_similarity(259, True)\n",
    "display_character_similarity(265, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Doc2Vec algorithm starts by giving distinct document-IDs an initial random vector; also most training modes include some randomized steps. So even identical runs-of-words won't necessarily result in identically-trained vectors. Rather, they'll tend to become closer over training – perhaps arbitrarily close with enough passes, but never identical. - https://groups.google.com/forum/#!topic/gensim/LLmPa4LECXs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}